{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Created by: [SmirkyGraphs](http://smirkygraphs.github.io/). Code: [Github](https://github.com/SmirkyGraphs/Python-Notebooks). Source: [insideairbnb.com](http://insideairbnb.com/get-the-data.html).\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rhode Island Airbnb Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains code used to combine all data from insideairbnb.com. Combining all data collected will ensure that we get not only all listings, but any that may have \"dropped-out\", and no longer list their location on airbnb. Additionally adding some wanted data to the reviews, such as city and review sentiment score using textblob.\n",
    "\n",
    "Tableau project based off the final cleaned dataset: [here](https://ivizri.com/posts/2020/07/rhode-island-airbnb/).\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_csv(files, filter_cols=None):\n",
    "    data = []\n",
    "    for f in files:\n",
    "        df = pd.read_csv(f)\n",
    "        df['file_name'] = str(f.parent)[-10:]\n",
    "        data.append(df)\n",
    "        \n",
    "    # combine files\n",
    "    df = pd.concat(data, sort=True)\n",
    "    \n",
    "    # convert filename to datetime & sort values \n",
    "    df['file_name'] = df['file_name'].apply(lambda x: pd.datetime.strptime(x, '%m_%d_%Y'))\n",
    "    df = df.sort_values('file_name')\n",
    "    \n",
    "    # filter columns for duplicates\n",
    "    if filter_cols == None:\n",
    "        filter_cols = [x for x in list(df) if x != 'file_name']\n",
    "    df = df.drop_duplicates(subset=filter_cols, keep='last')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "want_cols = [\n",
    "    'id',\n",
    "    'host_id',\n",
    "    'host_acceptance_rate',\n",
    "    'host_is_superhost',\n",
    "    'host_response_rate',\n",
    "    'host_response_time',\n",
    "    'host_since',\n",
    "    'host_location',\n",
    "    'property_type',\n",
    "    'room_type',\n",
    "    'accommodates',\n",
    "    'bathrooms',\n",
    "    'bedrooms',\n",
    "    'neighbourhood_cleansed',\n",
    "    'neighbourhood_group_cleansed',\n",
    "    'zipcode',\n",
    "    'latitude',\n",
    "    'longitude',\n",
    "    'price',\n",
    "    'security_deposit',\n",
    "    'cleaning_fee',\n",
    "    'number_of_reviews',\n",
    "    'number_of_reviews_ltm',\n",
    "    'first_review',\n",
    "    'last_review',\n",
    "    'review_scores_rating',\n",
    "    'availability_365',\n",
    "    'bed_type',\n",
    "    'file_name',\n",
    "    'name',\n",
    "    'host_name',\n",
    "    'minimum_nights'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine & clean all listings\n",
    "listing_files = Path('./data/raw/').glob('**/listings.csv')\n",
    "list_df = combine_csv(listing_files, filter_cols=['id'])\n",
    "list_df = list_df[want_cols]\n",
    "\n",
    "# filter for not_ri listings\n",
    "list_df['in_ri'] = list_df['host_location'].apply(\n",
    "lambda x: \n",
    "    'rhode island' in str(x).lower()\n",
    "    or ', ri' in str(x).lower() \n",
    "    or ' ri' in str(x).lower()\n",
    "    or str(x).lower() == 'nan'\n",
    "    or str(x).lower().strip() == 'usa'\n",
    "    or str(x).lower().strip() == 'us'\n",
    "    or str(x).lower() == 'ri'\n",
    "    or str(x).lower() == 'united states'\n",
    ")\n",
    "\n",
    "list_df.to_csv('./data/clean/listings_clean.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blob_sentiment(review):\n",
    "    review = str(review)\n",
    "    rev = TextBlob(review)\n",
    "    \n",
    "    if rev.sentiment.polarity > 0.02:\n",
    "        return 'positive'\n",
    "    elif rev.sentiment.polarity < -0.02:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'neutral'\n",
    "    \n",
    "# combine & clean all reviews\n",
    "review_files = Path('./data/raw/').glob('**/reviews.csv')\n",
    "rev_df = combine_csv(review_files, filter_cols=['id'])\n",
    "\n",
    "# remove null comments & automatic postings\n",
    "rev_df = rev_df[~rev_df['comments'].isnull()]\n",
    "rev_df = rev_df[~rev_df['comments'].str.contains('This is an automated posting.')]\n",
    "\n",
    "# join listing by city\n",
    "list_df = list_df.rename(columns={'id': 'listing_id'})\n",
    "rev_df = rev_df.merge(list_df[['listing_id', 'neighbourhood_cleansed']], how='left', on='listing_id')\n",
    "\n",
    "# add sentiment from textblob\n",
    "rev_df['blob_sentiment'] = rev_df['comments'].apply(blob_sentiment)\n",
    "rev_df.to_csv('./data/clean/reviews_clean.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
